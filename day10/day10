##Summary of day10
今天主要讲了两个问题：
1. 过拟合
2. 超参数的设定

1) 过拟合的问题出现有多种情况，其中常见的原因有：
1. 参数设计的过于多，模型的泛化不好
2. 训练集的样本太小
这里有一个重点是，始终要考虑样本的大小（尽可能多）,和神经网络学些的动态性

2)关于超参的设定，我们有两个策略：
1. 选定一个优化的范围（e.g. range(-0.001， 1000)）,观察图形的学习效果，反向优化超参数的选择
2. 贝叶斯


#### 机器学习的一个主要问题：过拟合
过拟合的问题出现有多种情况，其中常见的原因有：
1. 参数设计的过于多，模型的泛化不好
2. 训练集的样本太小

```code
#Assume: 
x_train.shape = (300, 784)

#then we, cut off the scope of trainning data/label:
x_train = x_train[:300]
t_train = t_train[:300]
```

#### 应对过拟合的策略
> 优化w（权值），或者优化神经网络自身

所以对应的思路出现的解决方案就是：
- 权值优化（针对W）
- droupout （针对网络自身）

关于droupout，其实本质上是对于神经网络的动态调整

#### 超参数（hyper-parameter）的验证
1. 需调用验证数据
2. 最优化问题 

关于超参的最优化问题：可以使用循环测试，使用验证数据不断做范围调整(依靠对图形的观察)的方法来优化超参数。
另：用贝叶斯方法是一个更精确和推荐的方法
